# -*- coding: utf-8 -*-
"""Pengembangan_Machine_Learning_Proyek_klasifikasi_gambar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L0MAKWfGlbK3gEDwn-7qDOcF8lRGsEZ0

## **Proyek Akhir: Klasifikasi gambar**
Pengembangan Machine learning

# Data diri

> **Nama : Muhammad Faqih Hakim**

> **Alamat : Kabupaten Tangerang**

> **Email: mhmdfkih21@gmail.com:**

> **Nomor Telpon: +6281292126827**

# **kriteria**
- **Bebas Memilih Dataset yang Ingin Dipakai, tetapi Harus Memiliki Minimal 1000 Gambar**
- **Tidak Diperbolehkan Menggunakan Dataset Rock, Paper, Scissor**
- **Dataset Dibagi Menjadi 80% Train Set dan 20% Test Set**
- **Model Harus Menggunakan Model Sequential, Conv2D, Pooling Layer**
- **Akurasi pada Training dan Validation Set Minimal Sebesar 85%**
- **Membuat Plot Terhadap Akurasi dan Loss Model**
- **Menyimpan Model ke Dalam Format Saved Model, TF-Lite dan TFJS**

# **Saran**


-  **Mengimplementasikan Callback**
- Gambar-gambar pada dataset memiliki resolusi yang tidak seragam.
- **Dataset yang digunakan berisi lebih dari 10000 gambar.**
- Akurasi pada training set dan validation set minimal 95%.
- **Memiliki 3 buah kelas atau lebih.**
- Melakukan inference menggunakan salah satu model (TF-Lite, TFJS atau savedmodel dengan tf serving).

## **Import Dataset**
"""

import zipfile
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.regularizers import l2
import glob
from google.colab import files
import shutil
import numpy as np
from PIL import Image
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pathlib

"""## **Load Dataset**

**Download dataset**

Tidak Diperbolehkan Menggunakan Dataset Rock, Paper, Scissor

> Link Dataset: https://www.kaggle.com/datasets/hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images
"""

!pip install kaggle

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images

"""**Ekstrak Dataset**

buat directory data untuk menyimpan
"""

extract_dir = 'data/'
os.makedirs(extract_dir, exist_ok=True)

"""ekstrak dan simpan kedalam directory yang sudah dibuat sebelumnya"""

zip_file = 'shoe-vs-sandal-vs-boot-dataset-15k-images.zip'
data_zip = zipfile.ZipFile(zip_file, 'r')
data_zip.extractall(extract_dir)
data_zip.close()
extract_dir = 'data/Shoe vs Sandal vs Boot Dataset'
print(f"File ZIP '{zip_file}' telah diekstrak ke dalam direktori '{extract_dir}'")

""""**Bebas Memilih Dataset yang Ingin Dipakai, tetapi Harus Memiliki Minimal 1000 Gambar**"

saran: Dataset yang digunakan berisi lebih dari 10000 gambar.
"""

def count_images(root_dir):
    image_counts = defaultdict(int)
    for root, dirs, files in os.walk(root_dir):
        data = [f for f in files if f.endswith('.jpg')]
        if data:
            category = os.path.basename(root)
            image_counts[category] += len(data)
    return image_counts

image_counts = count_images(extract_dir)

for category, count in image_counts.items():
    print(f"Jumlah gambar '{category}': {count}")

total_images = sum(image_counts.values())
print(f"Total jumlah gambar keseluruhan: {total_images}")

"""## **Data Preparation**

**plot distribusi antar kelas**

disini terlihat bahwa data terdistribusi dengan sangat baik

Memiliki 3 buah kelas atau lebih.
"""

file_name = []
labels = []
full_path = []

for path, subdirs, files in os.walk(extract_dir):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path": full_path, 'file_name': file_name, "labels": labels})

plt.figure(figsize=(8, 6))
sns.set_style("darkgrid")
plot_data = sns.countplot(x='labels', data=distribution_train)
plot_data.set_title('Distribution of Images in Each Class')
plot_data.set_xticklabels(plot_data.get_xticklabels(), rotation=45)
plt.ylabel('Number of Images')
plt.tight_layout()
plt.show()

"""Dataset Checking"""

categories = {
    'Boot': 'Boot',
    'Sandal': 'Sandal',
    'Shoe': 'Shoe'
}

fig, axs = plt.subplots(len(categories), 1, figsize=(15, 15))

for i, (class_name, subdir_name) in enumerate(categories.items()):
    class_path = os.path.join(extract_dir, subdir_name)
    image_files = [f for f in os.listdir(class_path) if f.endswith('.jpg')]
    if image_files:
        image_file = image_files[0]
        img_path = os.path.join(class_path, image_file)
        img = Image.open(img_path).convert("L")
        axs[i].imshow(img, cmap='gray')
        axs[i].set_title(class_name)
        axs[i].axis('off')

fig.tight_layout()
plt.show()

img_path = os.path.join(extract_dir, class_name, image_file)

"""**Split dataset**

simpan data menjadi sebuah dataFrame
"""

df = pd.DataFrame({"path": full_path, 'file_name': file_name, "labels": labels})
df

"""Memisahkan label dan membagi dataset menjadi train dan test




> **Dataset Dibagi Menjadi 80% Train Set dan 20% Test Set**


"""

for category in categories:
    class_path = os.path.join(extract_dir, category)
    if os.path.isdir(class_path):
        images = [f for f in os.listdir(class_path) if f.endswith('.jpg')]
        train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)

        train_dir = os.path.join('data_augmented', 'train', category)
        test_dir = os.path.join('data_augmented', 'test', category)
        os.makedirs(train_dir, exist_ok=True)
        os.makedirs(test_dir, exist_ok=True)

        for img in train_images:
            shutil.copy2(os.path.join(class_path, img), os.path.join(train_dir, img))

        for img in test_images:
            shutil.copy2(os.path.join(class_path, img), os.path.join(test_dir, img))

"""mendefinisikan directory train dan test"""

train_dir = 'data_augmented/train'
test_dir = 'data_augmented/test'

"""## **Augmentasi data**

menormalisasikan data
"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(136, 102),
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

validation_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(136, 102),
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(136, 102),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

"""## **Membangun model yang  Menggunakan Model Sequential, Conv2D, Pooling Layer**"""

model = tf.keras.models.Sequential([
     tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(136, 102, 3)),
     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
     tf.keras.layers.Dropout(0.25),
     tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
     tf.keras.layers.Dropout(0.25),

     tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),
     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
     tf.keras.layers.Dropout(0.25),

     tf.keras.layers.GlobalAveragePooling2D(),

     tf.keras.layers.Dense(512, activation='relu'),
     tf.keras.layers.Dropout(0.5),

     tf.keras.layers.Dense(256, activation='relu'),
     tf.keras.layers.Dropout(0.5),
     tf.keras.layers.Dense(3, activation='softmax')
])

"""Cek arsitektur model"""

model.summary()

"""Compaile model yang sudah dibuat"""

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""# **Mengimplementasikan Callback**

Buat fungsi callback yang memanggil model saat sendang dilatih ketika akurasi sudah sesuai yang diinginkan
"""

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy') >= 0.92 and logs.get('accuracy') >= 0.92):
            print("\nPelatihan harus dihentikan karena Sudah mencapai target yang diinginkan")
            self.model.stop_training = True

callbacks = myCallback()

"""mendefinisikan variabel untuk mengatur epoch dan lain lain"""

Epoch_count = 30
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)

"""Latih Model"""

history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=Epoch_count,
    callbacks=[callbacks, reduce_lr]
)

"""## **Plot Akurasi Model**"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('history accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

"""## **Plot Loss Model**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('history loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

"""## Menyimpan model ke dalam savedModel, TF-lite, TFJS
---


"""

import tensorflow as tf
save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

"""## Convert SavedModel menjadi vegs.tflite


"""

converter = tf.lite.TFLiteConverter.from_saved_model(save_path)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('shoes.tflite')
tflite_model_file.write_bytes(tflite_model)

"""## Save model to TensorflowJs Untuk Dijalankan di Web"""

model.save("model.h5")

"""install Tensorflof Js"""

!pip install tensorflowjs

"""Konversi model ke tfjs"""

!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

"""Terakhir download MyModel yang sudah disimpan sebelumnya dengan cara kita kompres terlebih dahulu

save model
"""

zip_filename = 'mymodel.zip'
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk(save_path):
        for file in files:
            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), save_path))

"""TFJS-model"""

zip_filename = 'tfjs_model.zip'
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk('tfjs_model'):
        for file in files:
            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), 'tfjs_model'))